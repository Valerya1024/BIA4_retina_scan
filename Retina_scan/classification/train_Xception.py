# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1limL-EjpEQBvVQl4rw85LIc3inufJSDg
"""

import keras.applications.xception
from keras.applications.xception import preprocess_input
from keras.preprocessing.image import ImageDataGenerator
from keras.layers import Dense, Activation, Flatten, Dropout, GlobalAveragePooling2D
from keras.models import Sequential, Model 
#from keras.optimizers import SGD, Adam
from keras.callbacks import TensorBoard
import keras
import matplotlib.pyplot as plt
import tensorflow as tf
from numpy import asarray
from PIL import Image
import pandas as pd
import imageio
import numpy as np
from skimage.transform import resize
'''
TRAIN_DIR = '/content/drive/MyDrive/ORIGA/ORIGA/cropped/Train'
VAL_DIR = '/content/drive/MyDrive/ORIGA/ORIGA/cropped/Validation'
'''
#TRAIN_DIR = '/content/drive/MyDrive/Fundus_Train_Val_Data/Fundus_Scanes_Sorted/Train'
#VAL_DIR = '/content/drive/MyDrive/Fundus_Train_Val_Data/Fundus_Scanes_Sorted/Validation'

#image = Image.open('C:/Users/jxm72/Desktop/semester7/BIA4/ICA1/Fundus_Train_Val_Data/Fundus_Scanes_Sorted/Validation/Glaucoma_Positive/613.jpg')
# summarize some details about the image
#print(image.format)
#print(image.mode)
#print(image.size)
HEIGHT = 300
WIDTH = 300
class_list = ["class_1", "class_2"]
FC_LAYERS = [1024, 512, 256]
dropout = 0.4
NUM_EPOCHS = 25
BATCH_SIZE = 8
#train_label = pd.read_csv('C:/Users/jxm72/Desktop/semester7/BIA4/ICA1/glaucoma.csv')
#y_train = train_label['Glaucoma']
#train_label.head()

def build_model(base_model, dropout, fc_layers, num_classes):
    for layer in base_model.layers:
        layer.trainable = False

    x = base_model.output
    x = Flatten()(x)
    for fc in fc_layers:
        print(fc)
        x = Dense(fc, activation='relu')(x)
        x = Dropout(dropout)(x)
    preditions = Dense(num_classes, activation='softmax')(x)
    finetune_model = Model(inputs = base_model.input, outputs = preditions)
    return finetune_model
base_model_1 = tf.keras.applications.Xception(weights = 'imagenet',
                       include_top = False,
                       input_shape = (HEIGHT, WIDTH, 3))

train_datagen = ImageDataGenerator(preprocessing_function = preprocess_input,
                                   rotation_range = 90,
                                   horizontal_flip = True,
                                   vertical_flip = True,
                                   width_shift_range=0.2,
                                   height_shift_range=0.2,
                                   zoom_range=0.1,)

val_datagen = ImageDataGenerator(preprocessing_function = preprocess_input,
                                  rotation_range = 90,
                                  horizontal_flip = True,
                                  vertical_flip = False)
'''
train_generator = train_datagen.flow_from_directory(TRAIN_DIR,
                                                    class_mode='categorical',
                                                    target_size = (HEIGHT, WIDTH),
                                                    batch_size = BATCH_SIZE)

val_generator = val_datagen.flow_from_directory(VAL_DIR,
                                                  class_mode='categorical',
                                                  target_size = (HEIGHT, WIDTH),
                                                  batch_size = BATCH_SIZE)


'''

model = build_model(base_model_1,
                                      dropout = dropout,
                                      fc_layers = FC_LAYERS,
                                      num_classes = len(class_list))
model.compile(optimizer='adam', loss="binary_crossentropy", metrics=["accuracy"])

filepath = "./model_weights.h5"
checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor = ["acc"], verbose= 1, mode = "max")
cb=TensorBoard(log_dir=("/content/drive/MyDrive/ORIGA"))
callbacks_list = [checkpoint, cb]
#print(train_generator.class_indices)
model.summary()
'''
early_stop = tf.keras.callbacks.EarlyStopping(monitor="loss", patience=3)

history = model.fit(train_generator, epochs = NUM_EPOCHS, steps_per_epoch = 25, 
                                       shuffle = True,validation_data=val_generator,
                    callbacks=[early_stop])

_, ax = plt.subplots(ncols=2, nrows=1, figsize=(8, 3))
history=history.history
ax[0].plot(history['loss'], c='gray', label = "Training loss")
ax[0].plot(history['val_loss'], c='red', label = "Validation loss")
ax[0].set_xlabel("Epoch")
ax[0].set_ylabel("Loss")
ax[0].set_ylim(0, 5)
ax[0].legend()

ax[1].plot(history['accuracy'], c='gray', label="Training accuracy")
ax[1].plot(history['val_accuracy'], c='red', label = "Validation accuracy")
ax[1].set_xlabel("Epoch")
ax[1].set_ylabel("Accuracy")
ax[1].legend()

model.save("/content/drive/MyDrive/model3")
np.save("/content/drive/MyDrive/model3_history.npy",history)'''